{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeef7b82"
      },
      "source": [
        "# Task\n",
        "Implement k-Nearest Neighbors (k-NN), Decision Tree (for Random Forest), Random Forest, and Naive Bayes classifiers from scratch, train and tune them using the provided `X_train`, `y_train`, `X_val`, `y_val` datasets, evaluate their performance on `X_test`, `y_test` using metrics like Accuracy, Precision, Recall, F1-Score, Confusion Matrix, and ROC-AUC, generate relevant visualizations including learning curves, ROC curves, and feature importance plots, and finally, summarize the findings and comparisons of these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e527afa"
      },
      "source": [
        "## Implement k-NN Classifier\n",
        "\n",
        "### Subtask:\n",
        "Implement a k-Nearest Neighbors (k-NN) classifier from scratch. This implementation should support both Euclidean and Manhattan distance metrics, incorporate weighted voting based on distance, and allow experimentation with various 'k' values (1, 3, 5, 7, 10).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31e05031"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the basic structure of the KNNClassifier class with an `__init__` method to initialize key parameters like `k` and `distance_metric` as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99dfbeeb"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        print(f\"KNNClassifier initialized with k={self.k} and distance_metric='{self.distance_metric}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eadba067"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `_euclidean_distance` and `_manhattan_distance` methods within the `KNNClassifier` class as private helper functions to calculate the distances between two data points, which are required for the k-NN algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0af2e1ef"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        print(f\"KNNClassifier initialized with k={self.k} and distance_metric='{self.distance_metric}'\")\n",
        "\n",
        "    def _euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def _manhattan_distance(self, x1, x2):\n",
        "        return np.sum(np.abs(x1 - x2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a581d755"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `fit` method within the `KNNClassifier` class to store the training data and labels, which is a necessary step before making predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ea7d58f"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        print(f\"KNNClassifier initialized with k={self.k} and distance_metric='{self.distance_metric}'\")\n",
        "\n",
        "    def _euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def _manhattan_distance(self, x1, x2):\n",
        "        return np.sum(np.abs(x1 - x2))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(\"KNNClassifier fit with training data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68b3a1ad"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `predict` method within the `KNNClassifier` class, which will handle calculating distances to training points, identifying the k-nearest neighbors, and performing weighted voting to determine the predicted class for each test data point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eb4e5e0"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        print(f\"KNNClassifier initialized with k={self.k} and distance_metric='{self.distance_metric}'\")\n",
        "\n",
        "    def _euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def _manhattan_distance(self, x1, x2):\n",
        "        return np.sum(np.abs(x1 - x2))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(\"KNNClassifier fit with training data.\")\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for x_test_point in X_test:\n",
        "            distances = []\n",
        "            for i, x_train_point in enumerate(self.X_train):\n",
        "                if self.distance_metric == 'euclidean':\n",
        "                    dist = self._euclidean_distance(x_test_point, x_train_point)\n",
        "                elif self.distance_metric == 'manhattan':\n",
        "                    dist = self._manhattan_distance(x_test_point, x_train_point)\n",
        "                else:\n",
        "                    raise ValueError(\"Unsupported distance metric. Choose 'euclidean' or 'manhattan'.\")\n",
        "                distances.append((dist, self.y_train.iloc[i]))\n",
        "\n",
        "            distances.sort(key=lambda x: x[0])\n",
        "            k_nearest_neighbors = distances[:self.k]\n",
        "\n",
        "            # Weighted voting\n",
        "            class_votes = Counter()\n",
        "            for dist, label in k_nearest_neighbors:\n",
        "                weight = 1.0 / (dist + 1e-6)  # Add a small constant to avoid division by zero\n",
        "                class_votes[label] += weight\n",
        "\n",
        "            if class_votes:\n",
        "                predicted_class = max(class_votes, key=class_votes.get)\n",
        "            else:\n",
        "                # Fallback if no neighbors or weights, e.g., predict the most frequent class in training data\n",
        "                # This case might happen if k=0 or k_nearest_neighbors is empty (which shouldn't happen with proper k)\n",
        "                predicted_class = Counter(self.y_train).most_common(1)[0][0]\n",
        "\n",
        "            predictions.append(predicted_class)\n",
        "        return np.array(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c13947f"
      },
      "source": [
        "## Implement Decision Tree for Random Forest\n",
        "\n",
        "### Subtask:\n",
        "Implement a basic Decision Tree algorithm from scratch, which will serve as the building block for the Random Forest. It should support Gini impurity and entropy as split criteria for node decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13980de5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in implementing the Decision Tree is to define the `TreeNode` class, which will serve as the fundamental structure for each node in the tree, storing essential attributes like feature index, threshold, children, and impurity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d183b491"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9a9ccd9"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `DecisionTree` class with its `__init__` method to initialize key parameters such as `max_depth`, `min_samples_split`, and `criterion`, as specified in the instructions for building the decision tree.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01a36843"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b87c4861"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement private helper methods, `_gini` and `_entropy`, within the `DecisionTree` class to calculate Gini impurity and entropy, respectively, for a given set of labels, which are essential for determining the best split criterion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4267b59"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0: # Handle empty set of labels\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        gini = 1.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            gini -= probability**2\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        entropy = 0.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            entropy -= probability * np.log2(probability)\n",
        "        return entropy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdcb0cc1"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a private helper method, `_find_best_split`, within the `DecisionTree` class. This method will iterate through all features and possible threshold values to determine the optimal split point that yields the lowest impurity (or highest information gain) based on the chosen criterion, which is a fundamental step in building the decision tree.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "696aabdd"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0: # Handle empty set of labels\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        gini = 1.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            gini -= probability**2\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        entropy = 0.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            entropy -= probability * np.log2(probability)\n",
        "        return entropy\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        best_gini = float('inf')\n",
        "        best_entropy = float('inf')\n",
        "        best_split = None\n",
        "\n",
        "        current_impurity = 0.0\n",
        "        if self.criterion == 'gini':\n",
        "            current_impurity = self._gini(y)\n",
        "        else:\n",
        "            current_impurity = self._entropy(y)\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                # Split data based on the threshold\n",
        "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_left, y_right = y[left_indices], y[right_indices]\n",
        "\n",
        "                if self.criterion == 'gini':\n",
        "                    gini_left = self._gini(y_left)\n",
        "                    gini_right = self._gini(y_right)\n",
        "\n",
        "                    # Weighted average of Gini impurities\n",
        "                    weighted_gini = (len(y_left) / n_samples) * gini_left + \\\n",
        "                                    (len(y_right) / n_samples) * gini_right\n",
        "\n",
        "                    if weighted_gini < best_gini:\n",
        "                        best_gini = weighted_gini\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices\n",
        "                        }\n",
        "                elif self.criterion == 'entropy':\n",
        "                    entropy_left = self._entropy(y_left)\n",
        "                    entropy_right = self._entropy(y_right)\n",
        "\n",
        "                    # Weighted average of entropy\n",
        "                    weighted_entropy = (len(y_left) / n_samples) * entropy_left + \\\n",
        "                                       (len(y_right) / n_samples) * entropy_right\n",
        "\n",
        "                    if weighted_entropy < best_entropy:\n",
        "                        best_entropy = weighted_entropy\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices\n",
        "                        }\n",
        "        return best_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8968faa9"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a private helper method, `_build_tree`, that recursively constructs the decision tree. This method should handle stopping conditions such as `max_depth` reached, `min_samples_split` not met, or all samples belonging to the same class, as it's the core logic for tree construction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "092f2269"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        # print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0: # Handle empty set of labels\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        gini = 1.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            gini -= probability**2\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        entropy = 0.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            entropy -= probability * np.log2(probability)\n",
        "        return entropy\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples <= 1: # Cannot split if less than or equal to 1 sample\n",
        "            return None\n",
        "\n",
        "        best_gain = -1.0 # Maximize information gain\n",
        "        best_split = None\n",
        "\n",
        "        current_impurity = 0.0\n",
        "        if self.criterion == 'gini':\n",
        "            current_impurity = self._gini(y)\n",
        "        else:\n",
        "            current_impurity = self._entropy(y)\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_left, y_right = y[left_indices], y[right_indices]\n",
        "                n_left, n_right = len(y_left), len(y_right)\n",
        "\n",
        "                if self.criterion == 'gini':\n",
        "                    impurity_left = self._gini(y_left)\n",
        "                    impurity_right = self._gini(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                        (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices\n",
        "                        }\n",
        "                elif self.criterion == 'entropy':\n",
        "                    impurity_left = self._entropy(y_left)\n",
        "                    impurity_right = self._entropy(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                       (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices\n",
        "                        }\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        # 1. If all samples belong to the same class\n",
        "        if n_labels == 1:\n",
        "            return TreeNode(value=y[0], impurity=0.0)\n",
        "\n",
        "        # 2. If max_depth is reached\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # 3. If min_samples_split is not met\n",
        "        if n_samples < self.min_samples_split:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._find_best_split(X, y)\n",
        "\n",
        "        # If no split improves impurity, make it a leaf node\n",
        "        if best_split is None or best_split.get('gain', 0) <= 0: # Assuming gain was calculated and returned\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Recursive calls to build left and right subtrees\n",
        "        left_child = self._build_tree(X[best_split['left_indices']], y[best_split['left_indices']], depth + 1)\n",
        "        right_child = self._build_tree(X[best_split['right_indices']], y[best_split['right_indices']], depth + 1)\n",
        "\n",
        "        # Calculate impurity for the current node based on the split\n",
        "        node_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "\n",
        "        return TreeNode(\n",
        "            feature_index=best_split['feature_index'],\n",
        "            threshold=best_split['threshold'],\n",
        "            left_child=left_child,\n",
        "            right_child=right_child,\n",
        "            impurity=node_impurity\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279c2118"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `fit` method that takes `X` (features) and `y` (labels) as input and calls `_build_tree` to construct the decision tree, which is necessary to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8267c5f5"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        # print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0: # Handle empty set of labels\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        gini = 1.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            gini -= probability**2\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        entropy = 0.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            entropy -= probability * np.log2(probability)\n",
        "        return entropy\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples <= 1: # Cannot split if less than or equal to 1 sample\n",
        "            return None\n",
        "\n",
        "        best_gain = -1.0 # Maximize information gain\n",
        "        best_split = None\n",
        "\n",
        "        current_impurity = 0.0\n",
        "        if self.criterion == 'gini':\n",
        "            current_impurity = self._gini(y)\n",
        "        else:\n",
        "            current_impurity = self._entropy(y)\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            # Use a limited number of unique values for thresholds to speed up computation\n",
        "            # Or consider a subset of feature values as potential thresholds\n",
        "            unique_values = np.unique(X[:, feature_index])\n",
        "            # For large datasets, sampling thresholds can be an option\n",
        "            # if len(unique_values) > self.max_thresholds_to_check:\n",
        "            #    thresholds = np.random.choice(unique_values, self.max_thresholds_to_check, replace=False)\n",
        "            # else:\n",
        "            thresholds = unique_values\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_left, y_right = y[left_indices], y[right_indices]\n",
        "                n_left, n_right = len(y_left), len(y_right)\n",
        "\n",
        "                if self.criterion == 'gini':\n",
        "                    impurity_left = self._gini(y_left)\n",
        "                    impurity_right = self._gini(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                        (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "                elif self.criterion == 'entropy':\n",
        "                    impurity_left = self._entropy(y_left)\n",
        "                    impurity_right = self._entropy(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                       (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        # 1. If all samples belong to the same class\n",
        "        if n_labels == 1:\n",
        "            return TreeNode(value=y[0], impurity=0.0)\n",
        "\n",
        "        # 2. If max_depth is reached\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # 3. If min_samples_split is not met\n",
        "        if n_samples < self.min_samples_split:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._find_best_split(X, y)\n",
        "\n",
        "        # If no split improves impurity (or no valid split found), make it a leaf node\n",
        "        if best_split is None or best_split.get('gain', 0) <= 0:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Recursive calls to build left and right subtrees\n",
        "        left_child = self._build_tree(X[best_split['left_indices']], y[best_split['left_indices']], depth + 1)\n",
        "        right_child = self._build_tree(X[best_split['right_indices']], y[best_split['right_indices']], depth + 1)\n",
        "\n",
        "        # Calculate impurity for the current node based on the split\n",
        "        node_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "\n",
        "        return TreeNode(\n",
        "            feature_index=best_split['feature_index'],\n",
        "            threshold=best_split['threshold'],\n",
        "            left_child=left_child,\n",
        "            right_child=right_child,\n",
        "            impurity=node_impurity\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Ensure X and y are numpy arrays for consistent indexing\n",
        "        self.X_train = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
        "        self.y_train = y.to_numpy() if hasattr(y, 'to_numpy') else y\n",
        "        self.root = self._build_tree(self.X_train, self.y_train)\n",
        "        print(\"DecisionTree fitted successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68088c55"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a private helper method, `_predict_one_sample`, within the `DecisionTree` class. This method will recursively traverse the trained tree from the root down to a leaf node, making decisions at each internal node based on the feature and threshold, and returning the predicted class value from the leaf node.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7333fa4"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        # print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0: # Handle empty set of labels\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        gini = 1.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            gini -= probability**2\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        entropy = 0.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            entropy -= probability * np.log2(probability)\n",
        "        return entropy\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples <= 1: # Cannot split if less than or equal to 1 sample\n",
        "            return None\n",
        "\n",
        "        best_gain = -1.0 # Maximize information gain\n",
        "        best_split = None\n",
        "\n",
        "        current_impurity = 0.0\n",
        "        if self.criterion == 'gini':\n",
        "            current_impurity = self._gini(y)\n",
        "        else:\n",
        "            current_impurity = self._entropy(y)\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            # Use a limited number of unique values for thresholds to speed up computation\n",
        "            # Or consider a subset of feature values as potential thresholds\n",
        "            unique_values = np.unique(X[:, feature_index])\n",
        "            # For large datasets, sampling thresholds can be an option\n",
        "            # if len(unique_values) > self.max_thresholds_to_check:\n",
        "            #    thresholds = np.random.choice(unique_values, self.max_thresholds_to_check, replace=False)\n",
        "            # else:\n",
        "            thresholds = unique_values\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_left, y_right = y[left_indices], y[right_indices]\n",
        "                n_left, n_right = len(y_left), len(y_right)\n",
        "\n",
        "                if self.criterion == 'gini':\n",
        "                    impurity_left = self._gini(y_left)\n",
        "                    impurity_right = self._gini(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                        (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "                elif self.criterion == 'entropy':\n",
        "                    impurity_left = self._entropy(y_left)\n",
        "                    impurity_right = self._entropy(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                       (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        # 1. If all samples belong to the same class\n",
        "        if n_labels == 1:\n",
        "            return TreeNode(value=y[0], impurity=0.0)\n",
        "\n",
        "        # 2. If max_depth is reached\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # 3. If min_samples_split is not met\n",
        "        if n_samples < self.min_samples_split:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._find_best_split(X, y)\n",
        "\n",
        "        # If no split improves impurity (or no valid split found), make it a leaf node\n",
        "        if best_split is None or best_split.get('gain', 0) <= 0:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Recursive calls to build left and right subtrees\n",
        "        left_child = self._build_tree(X[best_split['left_indices']], y[best_split['left_indices']], depth + 1)\n",
        "        right_child = self._build_tree(X[best_split['right_indices']], y[best_split['right_indices']], depth + 1)\n",
        "\n",
        "        # Calculate impurity for the current node based on the split\n",
        "        node_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "\n",
        "        return TreeNode(\n",
        "            feature_index=best_split['feature_index'],\n",
        "            threshold=best_split['threshold'],\n",
        "            left_child=left_child,\n",
        "            right_child=right_child,\n",
        "            impurity=node_impurity\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Ensure X and y are numpy arrays for consistent indexing\n",
        "        self.X_train = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
        "        self.y_train = y.to_numpy() if hasattr(y, 'to_numpy') else y\n",
        "        self.root = self._build_tree(self.X_train, self.y_train)\n",
        "        print(\"DecisionTree fitted successfully.\")\n",
        "\n",
        "    def _predict_one_sample(self, x, node):\n",
        "        # If it's a leaf node, return its value\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        # Traverse left or right based on the feature and threshold\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict_one_sample(x, node.left_child)\n",
        "        else:\n",
        "            return self._predict_one_sample(x, node.right_child)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69b6db6"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `predict` method within the `DecisionTree` class. This method will take an array of feature vectors `X_test` and return an array of predictions by iterating through each sample and calling the `_predict_one_sample` helper method for each.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be9ed629"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, value=None, impurity=None):\n",
        "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
        "        self.threshold = threshold          # Threshold value for splitting\n",
        "        self.left_child = left_child        # Left child node\n",
        "        self.right_child = right_child      # Right child node\n",
        "        self.value = value                  # Class value for leaf node (if not None, it's a leaf)\n",
        "        self.impurity = impurity            # Impurity of the node (e.g., Gini or entropy)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None # The root node of the decision tree\n",
        "        # print(f\"DecisionTree initialized with max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, and criterion='{self.criterion}'\")\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0: # Handle empty set of labels\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        gini = 1.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            gini -= probability**2\n",
        "        return gini\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0.0\n",
        "        counts = Counter(y)\n",
        "        entropy = 0.0\n",
        "        for label in counts:\n",
        "            probability = counts[label] / m\n",
        "            entropy -= probability * np.log2(probability)\n",
        "        return entropy\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples <= 1: # Cannot split if less than or equal to 1 sample\n",
        "            return None\n",
        "\n",
        "        best_gain = -1.0 # Maximize information gain\n",
        "        best_split = None\n",
        "\n",
        "        current_impurity = 0.0\n",
        "        if self.criterion == 'gini':\n",
        "            current_impurity = self._gini(y)\n",
        "        else:\n",
        "            current_impurity = self._entropy(y)\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            # Use a limited number of unique values for thresholds to speed up computation\n",
        "            # Or consider a subset of feature values as potential thresholds\n",
        "            unique_values = np.unique(X[:, feature_index])\n",
        "            # For large datasets, sampling thresholds can be an option\n",
        "            # if len(unique_values) > self.max_thresholds_to_check:\n",
        "            #    thresholds = np.random.choice(unique_values, self.max_thresholds_to_check, replace=False)\n",
        "            # else:\n",
        "            thresholds = unique_values\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
        "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
        "\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                y_left, y_right = y[left_indices], y[right_indices]\n",
        "                n_left, n_right = len(y_left), len(y_right)\n",
        "\n",
        "                if self.criterion == 'gini':\n",
        "                    impurity_left = self._gini(y_left)\n",
        "                    impurity_right = self._gini(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                        (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "                elif self.criterion == 'entropy':\n",
        "                    impurity_left = self._entropy(y_left)\n",
        "                    impurity_right = self._entropy(y_right)\n",
        "\n",
        "                    weighted_impurity = (n_left / n_samples) * impurity_left + \\\n",
        "                                       (n_right / n_samples) * impurity_right\n",
        "                    gain = current_impurity - weighted_impurity\n",
        "\n",
        "                    if gain > best_gain:\n",
        "                        best_gain = gain\n",
        "                        best_split = {\n",
        "                            'feature_index': feature_index,\n",
        "                            'threshold': threshold,\n",
        "                            'left_indices': left_indices,\n",
        "                            'right_indices': right_indices,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "        return best_split\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        # 1. If all samples belong to the same class\n",
        "        if n_labels == 1:\n",
        "            return TreeNode(value=y[0], impurity=0.0)\n",
        "\n",
        "        # 2. If max_depth is reached\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # 3. If min_samples_split is not met\n",
        "        if n_samples < self.min_samples_split:\n",
        "            # Create a leaf node with the most common class\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._find_best_split(X, y)\n",
        "\n",
        "        # If no split improves impurity (or no valid split found), make it a leaf node\n",
        "        if best_split is None or best_split.get('gain', 0) <= 0:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            leaf_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "            return TreeNode(value=leaf_value, impurity=leaf_impurity)\n",
        "\n",
        "        # Recursive calls to build left and right subtrees\n",
        "        left_child = self._build_tree(X[best_split['left_indices']], y[best_split['left_indices']], depth + 1)\n",
        "        right_child = self._build_tree(X[best_split['right_indices']], y[best_split['right_indices']], depth + 1)\n",
        "\n",
        "        # Calculate impurity for the current node based on the split\n",
        "        node_impurity = self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
        "\n",
        "        return TreeNode(\n",
        "            feature_index=best_split['feature_index'],\n",
        "            threshold=best_split['threshold'],\n",
        "            left_child=left_child,\n",
        "            right_child=right_child,\n",
        "            impurity=node_impurity\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Ensure X and y are numpy arrays for consistent indexing\n",
        "        self.X_train = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
        "        self.y_train = y.to_numpy() if hasattr(y, 'to_numpy') else y\n",
        "        self.root = self._build_tree(self.X_train, self.y_train)\n",
        "        print(\"DecisionTree fitted successfully.\")\n",
        "\n",
        "    def _predict_one_sample(self, x, node):\n",
        "        # If it's a leaf node, return its value\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        # Traverse left or right based on the feature and threshold\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict_one_sample(x, node.left_child)\n",
        "        else:\n",
        "            return self._predict_one_sample(x, node.right_child)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Ensure X is a numpy array for consistent indexing\n",
        "        X_np = X.to_numpy() if hasattr(X, 'to_numpy') else X\n",
        "        return np.array([self._predict_one_sample(x, self.root) for x in X_np])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15501cae"
      },
      "source": [
        "## Implement Random Forest Classifier\n",
        "\n",
        "### Subtask:\n",
        "Define the RandomForestClassifier class with an __init__ method to initialize parameters such as the number of estimators, maximum depth for individual trees, minimum samples required to split a node, impurity criterion, and a random state for reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22e97220"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `RandomForestClassifier` class with an `__init__` method to initialize parameters required for building a Random Forest, such as the number of estimators, max depth, min samples split, criterion, and a random state for reproducibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7f405c"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming TreeNode and DecisionTree classes are defined in previous cells\n",
        "# (or will be defined as part of the overall RandomForest implementation)\n",
        "\n",
        "class RandomForestClassifier:\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, criterion='gini', random_state=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.random_state = random_state\n",
        "        self.trees = [] # List to store individual DecisionTree objects\n",
        "        print(f\"RandomForestClassifier initialized with n_estimators={self.n_estimators}, max_depth={self.max_depth}, min_samples_split={self.min_samples_split}, criterion='{self.criterion}', and random_state={self.random_state}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}