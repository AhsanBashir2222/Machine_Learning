{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**POS TAGGING**"
      ],
      "metadata": {
        "id": "vZBRc9QHmRNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zszOi4eXlp6b",
        "outputId": "96ccd197-d347-452d-c2dc-5a027afa1609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"NLTK is a powerful library for natural language processing.\"\n",
        "words=word_tokenize(text)\n",
        "pos_tags=pos_tag(words)"
      ],
      "metadata": {
        "id": "6gQVdgucmDr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Original Text : \")\n",
        "print(text)\n",
        "print('\\n POS tagging :')\n",
        "for word,pos_tag in pos_tags:\n",
        "  print(f\"{word}:{pos_tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaqTD5KrmG-S",
        "outputId": "fcb1b377-eb97-46ef-c68e-9a549525f11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Original Text : \n",
            "NLTK is a powerful library for natural language processing.\n",
            "\n",
            " POS tagging :\n",
            "NLTK:NNP\n",
            "is:VBZ\n",
            "a:DT\n",
            "powerful:JJ\n",
            "library:NN\n",
            "for:IN\n",
            "natural:JJ\n",
            "language:NN\n",
            "processing:NN\n",
            ".:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "WNgkyMSrmcUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "word=\"Running\"\n",
        "stem=stemmer.stem(word)\n",
        "print(f\"Stemmed word :{stem}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unr2d8Agx93X",
        "outputId": "d1e73ea3-fc3d-4b6f-8a4a-337705eb3af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word :run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removel of special character**"
      ],
      "metadata": {
        "id": "6xQv9MVg1UZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s=\"Data!@Science#Rocks123\"\n",
        "res=\"\"\n",
        "\n",
        "for char in s:\n",
        "  if char.isalnum():\n",
        "    res+=char\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR8Pp4833Lf5",
        "outputId": "910d911e-f58a-44dd-f3a3-011e5ea2cc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataScienceRocks123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**null values:**"
      ],
      "metadata": {
        "id": "wG03ULFH4LOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'School ID': [101, 102, 103, np.nan, 105, 106, 107, 108],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Henry'],\n",
        "    'Address': ['123 Main St', '456 Oak Ave', '789 Pine Ln', '101 Elm St', np.nan, '222 Maple Rd', '444 Cedar Blvd', '555 Birch Dr'],\n",
        "    'City': ['Los Angeles', 'New York', 'Houston', 'Los Angeles', 'Miami', np.nan, 'Houston', 'New York'],\n",
        "    'Subject': ['Math', 'English', 'Science', 'Math', 'History', 'Math', 'Science', 'English'],\n",
        "    'Marks': [85, 92, 78, 89, np.nan, 95, 80, 88],\n",
        "    'Rank': [2, 1, 4, 3, 8, 1, 5, 3],\n",
        "    'Grade': ['B', 'A', 'C', 'B', 'D', 'A', 'C', 'B']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Sample DataFrame:\")\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG263HeM7mE8",
        "outputId": "62e3ef0a-a189-4f54-fafd-9039f2b0843a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample DataFrame:\n",
            "   School ID     Name         Address         City  Subject  Marks  Rank Grade\n",
            "0      101.0    Alice     123 Main St  Los Angeles     Math   85.0     2     B\n",
            "1      102.0      Bob     456 Oak Ave     New York  English   92.0     1     A\n",
            "2      103.0  Charlie     789 Pine Ln      Houston  Science   78.0     4     C\n",
            "3        NaN    David      101 Elm St  Los Angeles     Math   89.0     3     B\n",
            "4      105.0      Eva             NaN        Miami  History    NaN     8     D\n",
            "5      106.0    Frank    222 Maple Rd          NaN     Math   95.0     1     A\n",
            "6      107.0    Grace  444 Cedar Blvd      Houston  Science   80.0     5     C\n",
            "7      108.0    Henry    555 Birch Dr     New York  English   88.0     3     B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned=df.dropna()\n",
        "print(\"\\nDataFrame after removing rows with missing values:\")\n",
        "print(df_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59S8rkyJ7oP2",
        "outputId": "0c795355-1583-4cc5-bba7-b1ed0b6864a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after removing rows with missing values:\n",
            "   School ID     Name         Address         City  Subject  Marks  Rank Grade\n",
            "0      101.0    Alice     123 Main St  Los Angeles     Math   85.0     2     B\n",
            "1      102.0      Bob     456 Oak Ave     New York  English   92.0     1     A\n",
            "2      103.0  Charlie     789 Pine Ln      Houston  Science   78.0     4     C\n",
            "6      107.0    Grace  444 Cedar Blvd      Houston  Science   80.0     5     C\n",
            "7      108.0    Henry    555 Birch Dr     New York  English   88.0     3     B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_imputation=df['Marks'].fillna(df['Marks'].mean())\n",
        "median_imputation=df['Marks'].fillna(df['Marks'].median())\n",
        "print(\"\\nImputation using Mean:\")\n",
        "print(mean_imputation)\n",
        "print(\"\\nImputation using Median:\")\n",
        "print(median_imputation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L2DqskF8LS0",
        "outputId": "e58380e2-d387-4887-8223-1dfa45ca8627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputation using Mean:\n",
            "0    85.000000\n",
            "1    92.000000\n",
            "2    78.000000\n",
            "3    89.000000\n",
            "4    86.714286\n",
            "5    95.000000\n",
            "6    80.000000\n",
            "7    88.000000\n",
            "Name: Marks, dtype: float64\n",
            "\n",
            "Imputation using Median:\n",
            "0    85.0\n",
            "1    92.0\n",
            "2    78.0\n",
            "3    89.0\n",
            "4    88.0\n",
            "5    95.0\n",
            "6    80.0\n",
            "7    88.0\n",
            "Name: Marks, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction:**"
      ],
      "metadata": {
        "id": "4p9Z8g7i9dJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# FIX: Run the exact download command suggested by the error message\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords') # Keep this as well\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGfFWM6viz64",
        "outputId": "2fa20fef-4452-4e9a-f620-416976096756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown fox jumps over the lazy dog.\n",
            "Text after Stopword Removal: quick brown fox jumps lazy dog .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "new_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Now these functions should work!\n",
        "new_words = word_tokenize(new_text)\n",
        "new_filtered_words = [\n",
        "    word for word in new_words if word.lower() not in stopwords.words('english')]\n",
        "new_clean_text = ' '.join(new_filtered_words)\n",
        "\n",
        "print(\"Original Text:\", new_text)\n",
        "print(\"Text after Stopword Removal:\", new_clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BZ4-v94j58o",
        "outputId": "a311cf9b-add1-4994-f2b0-8d4eb4b3511f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown fox jumps over the lazy dog.\n",
            "Text after Stopword Removal: quick brown fox jumps lazy dog .\n"
          ]
        }
      ]
    }
  ]
}